In this paper, we identify the problem of learning from
{\em sparse} and {\em underspecified} rewards. We tackle this problem by employing
a mode covering exploration strategy and meta learning an auxiliary terminal
reward function without using any expert demonstrations.  
% There are two main challenges in this setting: (1) how to explore the combinatorial
% search space to find the rare successes; (2) how to differentiate
% accidental successes from purposeful successes. We propose to (1) use
% a mode covering direction of KL divergence for effective exploration
% and a mode seeking direction of KL divergence to train a robust policy; (2) learn
% a parametric auxiliary terminal reward function by optimizing the validation
% performance of the trained policy through Meta-Learning and Bayesian
% Optimization without using any expert demonstrations.
% We showed the effectiveness of our approach on a simple
% instruction following task and two weakly supervised
% semantic parsing benchmarks.

As future work, we'd like to extend our approach to
learn non-terminal auxiliary rewards as well as replace the linear reward model with more powerful models
such as neural networks. Another interesting direction is to improve upon the local optimization behavior
in MeRL via random restarts, annealing etc.